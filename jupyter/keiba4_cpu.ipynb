{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Horse racing prediction  \n",
    "\n",
    "This is an experiment to predict the outcome of horse racing based on past 5 race results, jockey, and trainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "\n",
    "db = client.keiba\n",
    "training_data = db.training_data_Kisyu_Kyusya_1_race_5_with_odds\n",
    "data_models = db.data_models_Kisyu_Kyusya_1_race_5_with_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# > db.training_data_Kisyu_Kyusya_1_race_5_with_odds.count({'input_x_count':105})\n",
    "# 9245046\n",
    "#\n",
    "# > db.training_data_Kisyu_Kyusya_1_race_5_with_odds.count()\n",
    "# 9247112\n",
    "#\n",
    "# for some reasons, 2,066 record don't have 105 features...\n",
    "# \n",
    "\n",
    "training_data_cursor = training_data.find({'random_index': {'$gt':19}}, no_cursor_timeout=True)\n",
    "validation_data_cursor = training_data.find({'random_index': {'$lt':20}}, no_cursor_timeout=True)\n",
    "\n",
    "# too slow to use\n",
    "# training_data_cursor = training_data.find({'random_index': {'$gt':19}, 'input_x_count':105})\n",
    "# validation_data_cursor = training_data.find({'random_index': {'$lt':20, 'input_x_count':105}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_count: 7488116\n",
      "validation_data_count: 1756930\n"
     ]
    }
   ],
   "source": [
    "training_data_count = training_data_cursor.count()\n",
    "print(\"training_data_count: {}\".format(training_data_count))\n",
    "\n",
    "validation_data_count = validation_data_cursor.count()\n",
    "print(\"validation_data_count: {}\".format(validation_data_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get std and mean. we use data_model later\n",
    "mean_and_std = data_models.find_one({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get batch as generator\n",
    "# batch_size is mini batch size\n",
    "# data_type is training or validation\n",
    "\n",
    "#\n",
    "# pymongo.errors.CursorNotFound: cursor id 'â€¦' not valid at server\n",
    "#\n",
    "\n",
    "# do not use try exception model,\n",
    "# can not chatch error if we use it.\n",
    "\n",
    "\n",
    "def data_generator(batch_size, data_type):\n",
    "    \n",
    "#     still_have_data_flg = True\n",
    "    \n",
    "    input_X = np.zeros(shape=(batch_size, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(batch_size, 1), dtype=float)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for idx1 in range(batch_size):\n",
    "            \n",
    "            # get one row\n",
    "            data1 = None\n",
    "            if data_type == 'validation':\n",
    "                data1 = validation_data_cursor.next()\n",
    "            else:\n",
    "                data1 = training_data_cursor.next()\n",
    "\n",
    "            # normalize x values\n",
    "            for idx2 in data1['input_x_object']:\n",
    "\n",
    "                # get model data which contains mean and std\n",
    "                x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "                mean_name = 'input_x_avg_'+idx2\n",
    "                mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "                std_name = 'input_x_std_'+idx2\n",
    "                std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "                normarized_x = (x1 - mean_value) / std_value\n",
    "                input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "            # normarize y value\n",
    "            y1 = data1['target_y']\n",
    "            y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "            y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "            normalized_y = (y1 - y_mean_value) / y_std_value\n",
    "\n",
    "            target_Y[idx1] = normalized_y\n",
    "\n",
    "            yield (input_X, target_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#  y is 1 or 0\n",
    "\n",
    "def data_generator_binary(batch_size, data_type):\n",
    "    \n",
    "#     still_have_data_flg = True\n",
    "    \n",
    "    input_X = np.zeros(shape=(batch_size, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(batch_size, 1), dtype=float)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for idx1 in range(batch_size):\n",
    "            \n",
    "            # get one row\n",
    "            data1 = None\n",
    "            if data_type == 'validation':\n",
    "                data1 = validation_data_cursor.next()\n",
    "            else:\n",
    "                data1 = training_data_cursor.next()\n",
    "\n",
    "            # normalize x values\n",
    "            for idx2 in data1['input_x_object']:\n",
    "\n",
    "                # get model data which contains mean and std\n",
    "                x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "                mean_name = 'input_x_avg_'+idx2\n",
    "                mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "                std_name = 'input_x_std_'+idx2\n",
    "                std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "                normarized_x = (x1 - mean_value) / std_value\n",
    "                input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "            # normarize y value\n",
    "            y1 = data1['target_y']\n",
    "            if y1 >= 0:\n",
    "                target_Y[idx1] = 1\n",
    "            else:\n",
    "                target_Y[idx1] = 0\n",
    "#             y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "#             y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "#             normalized_y = (y1 - y_mean_value) / y_std_value\n",
    "\n",
    "#             target_Y[idx1] = normalized_y\n",
    "\n",
    "            yield (input_X, target_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kouohhashi/anaconda3/envs/aind-vui/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import dependancies\n",
    "import keras\n",
    "from keras import metrics, initializers\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Dropout, Dense, LeakyReLU, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               13568     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 79,617\n",
      "Trainable params: 79,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 1: 3 layers, LeakyReLU, and dropout\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(128, input_shape=(105,), activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(256, activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(128, activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(1, activation=None))\n",
    "\n",
    "model_1.compile(optimizer='rmsprop',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[metrics.mae])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "234003/234003 [==============================] - 974s 4ms/step - loss: 0.2908 - mean_absolute_error: 0.2090 - val_loss: 1.3641 - val_mean_absolute_error: 0.4020\n",
      "Epoch 2/20\n",
      "234003/234003 [==============================] - 965s 4ms/step - loss: 0.3030 - mean_absolute_error: 0.2161 - val_loss: 0.9627 - val_mean_absolute_error: 0.2172\n",
      "Epoch 3/20\n",
      "234003/234003 [==============================] - 963s 4ms/step - loss: 0.2889 - mean_absolute_error: 0.2142 - val_loss: 1.0180 - val_mean_absolute_error: 0.2672\n",
      "Epoch 4/20\n",
      "234003/234003 [==============================] - 963s 4ms/step - loss: 0.3061 - mean_absolute_error: 0.2230 - val_loss: 1.0920 - val_mean_absolute_error: 0.3859\n",
      "Epoch 5/20\n",
      "234003/234003 [==============================] - 968s 4ms/step - loss: 0.3030 - mean_absolute_error: 0.2225 - val_loss: 1.4493 - val_mean_absolute_error: 0.5858\n",
      "Epoch 6/20\n",
      "234003/234003 [==============================] - 965s 4ms/step - loss: 0.3015 - mean_absolute_error: 0.2251 - val_loss: 1.0960 - val_mean_absolute_error: 0.3155\n",
      "Epoch 7/20\n",
      "234003/234003 [==============================] - 957s 4ms/step - loss: 0.3109 - mean_absolute_error: 0.2296 - val_loss: 1.5001 - val_mean_absolute_error: 0.5730\n",
      "Epoch 8/20\n",
      "234003/234003 [==============================] - 953s 4ms/step - loss: 0.3012 - mean_absolute_error: 0.2254 - val_loss: 1.2145 - val_mean_absolute_error: 0.3210\n",
      "Epoch 9/20\n",
      "234003/234003 [==============================] - 966s 4ms/step - loss: 0.3225 - mean_absolute_error: 0.2386 - val_loss: 1.0248 - val_mean_absolute_error: 0.2598\n",
      "Epoch 10/20\n",
      "234003/234003 [==============================] - 965s 4ms/step - loss: 0.3145 - mean_absolute_error: 0.2337 - val_loss: 1.2287 - val_mean_absolute_error: 0.3917\n",
      "Epoch 11/20\n",
      "234003/234003 [==============================] - 967s 4ms/step - loss: 0.3232 - mean_absolute_error: 0.2439 - val_loss: 1.4096 - val_mean_absolute_error: 0.6580\n",
      "Epoch 12/20\n",
      "234003/234003 [==============================] - 962s 4ms/step - loss: 0.3154 - mean_absolute_error: 0.2374 - val_loss: 1.0469 - val_mean_absolute_error: 0.2184\n",
      "Epoch 13/20\n",
      "234003/234003 [==============================] - 968s 4ms/step - loss: 0.3272 - mean_absolute_error: 0.2443 - val_loss: 1.0371 - val_mean_absolute_error: 0.2025\n",
      "Epoch 14/20\n",
      "234003/234003 [==============================] - 979s 4ms/step - loss: 0.3207 - mean_absolute_error: 0.2427 - val_loss: 2.3075 - val_mean_absolute_error: 0.6338\n",
      "Epoch 15/20\n",
      "234003/234003 [==============================] - 987s 4ms/step - loss: 0.3481 - mean_absolute_error: 0.2519 - val_loss: 1.5126 - val_mean_absolute_error: 0.4519\n",
      "Epoch 16/20\n",
      "234003/234003 [==============================] - 984s 4ms/step - loss: 0.3444 - mean_absolute_error: 0.2527 - val_loss: 1.4132 - val_mean_absolute_error: 0.5690\n",
      "Epoch 17/20\n",
      "234003/234003 [==============================] - 953s 4ms/step - loss: 0.3533 - mean_absolute_error: 0.2551 - val_loss: 1.0096 - val_mean_absolute_error: 0.1829\n",
      "Epoch 18/20\n",
      "234003/234003 [==============================] - 964s 4ms/step - loss: 0.3365 - mean_absolute_error: 0.2523 - val_loss: 1.3932 - val_mean_absolute_error: 0.4039\n",
      "Epoch 19/20\n",
      "234003/234003 [==============================] - 968s 4ms/step - loss: 0.3440 - mean_absolute_error: 0.2553 - val_loss: 0.9228 - val_mean_absolute_error: 0.1716\n",
      "Epoch 20/20\n",
      "234003/234003 [==============================] - 960s 4ms/step - loss: 0.3253 - mean_absolute_error: 0.2486 - val_loss: 1.2742 - val_mean_absolute_error: 0.4380\n",
      "{'_id': 'Kisyu_Kyusya_1_race_5_with_odds-20140126071120071027752007104975', 'data_name': 'Kisyu_Kyusya_1_race_5_with_odds', 'input_x': [1800, 3125, 7, 560, 500, -2, 0.1863905325443787, 0.10754414125200643, 1228, 7, 590, 502, -2, 15, 377, 10, 1800, 692, 6, 570, 504, 18, 12, 368, 7, 1800, 911, 6, 590, 486, -12, 12, 364, 8, 1800, 192, 6, 560, 498, -8, 15, 365, 9, 1700, 165, 6, 590, 506, 10, 16, 380, 8, 1700, 3125, 7, 560, 500, -2, 0.1683673469387755, 0.1669024045261669, 716, 7, 570, 518, -2, 22, 395, 13, 1800, 1664, 6, 560, 520, -2, 8, 343, 12, 1800, 978, 6, 560, 522, -8, 7, 331, 13, 1200, 128, 6, 570, 530, 4, 10, 351, 10, 1800, 814, 6, 570, 526, 6, 10, 364, 12, 1600], 'target_y': -2, 'input_x_object': {'0': 1800, '1': 3125, '2': 7, '3': 560, '4': 500, '5': -2, '6': 0.1863905325443787, '7': 0.10754414125200643, '8': 1228, '9': 7, '10': 590, '11': 502, '12': -2, '13': 15, '14': 377, '15': 10, '16': 1800, '17': 692, '18': 6, '19': 570, '20': 504, '21': 18, '22': 12, '23': 368, '24': 7, '25': 1800, '26': 911, '27': 6, '28': 590, '29': 486, '30': -12, '31': 12, '32': 364, '33': 8, '34': 1800, '35': 192, '36': 6, '37': 560, '38': 498, '39': -8, '40': 15, '41': 365, '42': 9, '43': 1700, '44': 165, '45': 6, '46': 590, '47': 506, '48': 10, '49': 16, '50': 380, '51': 8, '52': 1700, '53': 3125, '54': 7, '55': 560, '56': 500, '57': -2, '58': 0.1683673469387755, '59': 0.1669024045261669, '60': 716, '61': 7, '62': 570, '63': 518, '64': -2, '65': 22, '66': 395, '67': 13, '68': 1800, '69': 1664, '70': 6, '71': 560, '72': 520, '73': -2, '74': 8, '75': 343, '76': 12, '77': 1800, '78': 978, '79': 6, '80': 560, '81': 522, '82': -8, '83': 7, '84': 331, '85': 13, '86': 1200, '87': 128, '88': 6, '89': 570, '90': 530, '91': 4, '92': 10, '93': 351, '94': 10, '95': 1800, '96': 814, '97': 6, '98': 570, '99': 526, '100': 6, '101': 10, '102': 364, '103': 12, '104': 1600}, 'input_x_count': 105, 'createdAt': datetime.datetime(2017, 11, 15, 16, 23, 46, 805000), 'normalized': False, 'random_index': 12}\n",
      "{'_id': 'Kisyu_Kyusya_1_race_5_with_odds-20140126071120091026582007103864', 'data_name': 'Kisyu_Kyusya_1_race_5_with_odds', 'input_x': [1800, 493, 5, 560, 502, -2, 0.2222222222222222, 0.23787528868360278, 129, 5, 560, 504, 0, 4, 379, 6, 1800, 84, 4, 560, 504, 0, 6, 367, 5, 2000, 188, 4, 560, 504, 0, 11, 366, 6, 1800, 55, 4, 575, 504, 6, -2, 372, 1, 1800, 50, 4, 570, 498, 0, 0, 362, 1, 1800, 493, 5, 560, 502, -2, 0.16451612903225807, 0.16666666666666666, 1158, 6, 570, 492, -6, 8, 362, 6, 1800, 110, 6, 570, 498, 8, 5, 363, 5, 1800, 170, 6, 585, 490, -14, 2, 367, 3, 2000, 82, 6, 570, 504, -2, 7, 361, 5, 1700, 41, 6, 580, 498, 10, 0, 370, 1, 1800], 'target_y': -7, 'input_x_object': {'0': 1800, '1': 493, '2': 5, '3': 560, '4': 502, '5': -2, '6': 0.2222222222222222, '7': 0.23787528868360278, '8': 129, '9': 5, '10': 560, '11': 504, '12': 0, '13': 4, '14': 379, '15': 6, '16': 1800, '17': 84, '18': 4, '19': 560, '20': 504, '21': 0, '22': 6, '23': 367, '24': 5, '25': 2000, '26': 188, '27': 4, '28': 560, '29': 504, '30': 0, '31': 11, '32': 366, '33': 6, '34': 1800, '35': 55, '36': 4, '37': 575, '38': 504, '39': 6, '40': -2, '41': 372, '42': 1, '43': 1800, '44': 50, '45': 4, '46': 570, '47': 498, '48': 0, '49': 0, '50': 362, '51': 1, '52': 1800, '53': 493, '54': 5, '55': 560, '56': 502, '57': -2, '58': 0.16451612903225807, '59': 0.16666666666666666, '60': 1158, '61': 6, '62': 570, '63': 492, '64': -6, '65': 8, '66': 362, '67': 6, '68': 1800, '69': 110, '70': 6, '71': 570, '72': 498, '73': 8, '74': 5, '75': 363, '76': 5, '77': 1800, '78': 170, '79': 6, '80': 585, '81': 490, '82': -14, '83': 2, '84': 367, '85': 3, '86': 2000, '87': 82, '88': 6, '89': 570, '90': 504, '91': -2, '92': 7, '93': 361, '94': 5, '95': 1700, '96': 41, '97': 6, '98': 580, '99': 498, '100': 10, '101': 0, '102': 370, '103': 1, '104': 1800}, 'input_x_count': 105, 'createdAt': datetime.datetime(2017, 11, 15, 16, 23, 54, 209000), 'normalized': False, 'random_index': 12}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb05c083f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_1\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_1.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 32\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_1.fit_generator(generator=data_generator(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=20,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                6784      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,897\n",
      "Trainable params: 8,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 2: simple one with 2 layers, LeakyReLU and dropout\n",
    "model_2 = Sequential()\n",
    "\n",
    "# keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "# kernel_initializer=initializers.RandomNormal(stddev=0.001)\n",
    "model_2.add(Dense(64,\n",
    "                  input_shape=(105,), \n",
    "                  activation=None, \n",
    "                  bias_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                  kernel_initializer=initializers.RandomNormal(stddev=0.001)))\n",
    "model_2.add(LeakyReLU(alpha=0.2))\n",
    "model_2.add(Dropout(0.2))\n",
    "\n",
    "model_2.add(Dense(32, activation=None))\n",
    "model_2.add(LeakyReLU(alpha=0.2))\n",
    "model_2.add(Dropout(0.2))\n",
    "\n",
    "model_2.add(Dense(1, activation=None))\n",
    "\n",
    "keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "# SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model_2.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[metrics.mae])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234003/234003 [==============================] - 561s 2ms/step - loss: 0.9403 - mean_absolute_error: 0.3288 - val_loss: 0.9947 - val_mean_absolute_error: 0.1900\n",
      "Epoch 2/10\n",
      "234003/234003 [==============================] - 559s 2ms/step - loss: 1.1812 - mean_absolute_error: 0.3479 - val_loss: 1.3071 - val_mean_absolute_error: 0.4377\n",
      "Epoch 3/10\n",
      "234003/234003 [==============================] - 562s 2ms/step - loss: 1.1814 - mean_absolute_error: 0.3510 - val_loss: 0.9916 - val_mean_absolute_error: 0.2160\n",
      "Epoch 4/10\n",
      "234003/234003 [==============================] - 561s 2ms/step - loss: 1.2087 - mean_absolute_error: 0.3656 - val_loss: 0.9631 - val_mean_absolute_error: 0.2103\n",
      "Epoch 5/10\n",
      "234003/234003 [==============================] - 559s 2ms/step - loss: 1.0372 - mean_absolute_error: 0.3472 - val_loss: 1.2585 - val_mean_absolute_error: 0.3393\n",
      "Epoch 6/10\n",
      "234003/234003 [==============================] - 557s 2ms/step - loss: 1.2197 - mean_absolute_error: 0.3497 - val_loss: 0.9334 - val_mean_absolute_error: 0.2019\n",
      "Epoch 7/10\n",
      "234003/234003 [==============================] - 574s 2ms/step - loss: 1.7545 - mean_absolute_error: 0.3531 - val_loss: 1.1403 - val_mean_absolute_error: 0.3034\n",
      "Epoch 8/10\n",
      "234003/234003 [==============================] - 559s 2ms/step - loss: 1.0922 - mean_absolute_error: 0.3569 - val_loss: 1.5596 - val_mean_absolute_error: 0.5464\n",
      "Epoch 9/10\n",
      "234003/234003 [==============================] - 565s 2ms/step - loss: 1.0634 - mean_absolute_error: 0.3467 - val_loss: 0.9214 - val_mean_absolute_error: 0.2724\n",
      "Epoch 10/10\n",
      "234003/234003 [==============================] - 556s 2ms/step - loss: 1.0158 - mean_absolute_error: 0.3479 - val_loss: 1.2013 - val_mean_absolute_error: 0.3095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb05587bdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_2\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_2.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 32\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_2.fit_generator(generator=data_generator(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                6784      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 9,281\n",
      "Trainable params: 9,089\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 3: simple one  + batch normalization\n",
    "model_3 = Sequential()\n",
    "\n",
    "\n",
    "# keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "model_3.add(Dense(64,\n",
    "                  input_shape=(105,),\n",
    "                  activation=None,\n",
    "                  kernel_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(LeakyReLU(alpha=0.2))\n",
    "model_3.add(Dropout(0.2))\n",
    "\n",
    "model_3.add(Dense(32, activation=None))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(LeakyReLU(alpha=0.2))\n",
    "model_3.add(Dropout(0.2))\n",
    "\n",
    "model_3.add(Dense(1, activation=None))\n",
    "\n",
    "model_3.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[metrics.mae])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117001/117001 [==============================] - 418s 4ms/step - loss: 0.7740 - mean_absolute_error: 0.2722 - val_loss: 1.5539 - val_mean_absolute_error: 0.5312\n",
      "Epoch 2/10\n",
      "117001/117001 [==============================] - 405s 3ms/step - loss: 0.8763 - mean_absolute_error: 0.2767 - val_loss: 1.0532 - val_mean_absolute_error: 0.2857\n",
      "Epoch 3/10\n",
      "117001/117001 [==============================] - 402s 3ms/step - loss: 0.8814 - mean_absolute_error: 0.2769 - val_loss: 1.0289 - val_mean_absolute_error: 0.3160\n",
      "Epoch 4/10\n",
      "117001/117001 [==============================] - 408s 3ms/step - loss: 0.9084 - mean_absolute_error: 0.2758 - val_loss: 1.2050 - val_mean_absolute_error: 0.3504\n",
      "Epoch 5/10\n",
      "117001/117001 [==============================] - 408s 3ms/step - loss: 0.8514 - mean_absolute_error: 0.2671 - val_loss: 1.2065 - val_mean_absolute_error: 0.3319\n",
      "Epoch 6/10\n",
      "117001/117001 [==============================] - 405s 3ms/step - loss: 0.9047 - mean_absolute_error: 0.2732 - val_loss: 0.9973 - val_mean_absolute_error: 0.2168\n",
      "Epoch 7/10\n",
      "117001/117001 [==============================] - 405s 3ms/step - loss: 0.9346 - mean_absolute_error: 0.2766 - val_loss: 1.0766 - val_mean_absolute_error: 0.2379\n",
      "Epoch 8/10\n",
      "117001/117001 [==============================] - 409s 3ms/step - loss: 0.8830 - mean_absolute_error: 0.2608 - val_loss: 0.8789 - val_mean_absolute_error: 0.2210\n",
      "Epoch 9/10\n",
      "117001/117001 [==============================] - 405s 3ms/step - loss: 0.8993 - mean_absolute_error: 0.2628 - val_loss: 1.1398 - val_mean_absolute_error: 0.2316\n",
      "Epoch 10/10\n",
      "117001/117001 [==============================] - 407s 3ms/step - loss: 0.9188 - mean_absolute_error: 0.2684 - val_loss: 1.0035 - val_mean_absolute_error: 0.2166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74bd03e4a8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_3\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_3.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 64\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_3.fit_generator(generator=data_generator(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                6784      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 56,449\n",
      "Trainable params: 56,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 4\n",
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(Dense(64,\n",
    "                  input_shape=(105,),\n",
    "                  activation=None,\n",
    "                  kernel_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_4.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_4.add(Dense(256, activation=None))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_4.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_4.add(Dense(128, activation=None))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_4.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_4.add(Dense(1, activation=None))\n",
    "\n",
    "# SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)?\n",
    "# model_4.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "model_4.compile(optimizer=SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[metrics.mae])\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117001/117001 [==============================] - 461s 4ms/step - loss: 0.4133 - mean_absolute_error: 0.2107 - val_loss: 1.2420 - val_mean_absolute_error: 0.3082\n",
      "Epoch 2/10\n",
      "117001/117001 [==============================] - 456s 4ms/step - loss: 0.3529 - mean_absolute_error: 0.1918 - val_loss: 1.2498 - val_mean_absolute_error: 0.3263\n",
      "Epoch 3/10\n",
      "117001/117001 [==============================] - 463s 4ms/step - loss: 0.3750 - mean_absolute_error: 0.1863 - val_loss: 1.0277 - val_mean_absolute_error: 0.2752\n",
      "Epoch 4/10\n",
      "117001/117001 [==============================] - 455s 4ms/step - loss: 0.3851 - mean_absolute_error: 0.1852 - val_loss: 1.2904 - val_mean_absolute_error: 0.3192\n",
      "Epoch 5/10\n",
      "117001/117001 [==============================] - 462s 4ms/step - loss: 0.4198 - mean_absolute_error: 0.1916 - val_loss: 1.2641 - val_mean_absolute_error: 0.4141\n",
      "Epoch 6/10\n",
      "117001/117001 [==============================] - 461s 4ms/step - loss: 0.4226 - mean_absolute_error: 0.1918 - val_loss: 1.3638 - val_mean_absolute_error: 0.3052\n",
      "Epoch 7/10\n",
      "117001/117001 [==============================] - 455s 4ms/step - loss: 0.4160 - mean_absolute_error: 0.1819 - val_loss: 0.9540 - val_mean_absolute_error: 0.2351\n",
      "Epoch 8/10\n",
      "117001/117001 [==============================] - 452s 4ms/step - loss: 0.4533 - mean_absolute_error: 0.1905 - val_loss: 1.5048 - val_mean_absolute_error: 0.3120\n",
      "Epoch 9/10\n",
      "117001/117001 [==============================] - 456s 4ms/step - loss: 0.4392 - mean_absolute_error: 0.1828 - val_loss: 1.2197 - val_mean_absolute_error: 0.2929\n",
      "Epoch 10/10\n",
      "117001/117001 [==============================] - 463s 4ms/step - loss: 0.4503 - mean_absolute_error: 0.1840 - val_loss: 1.2588 - val_mean_absolute_error: 0.3216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd00ae8128>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_4\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_4.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 64\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_4.fit_generator(generator=data_generator(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## model 5 binary target_Y ( 0 or 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 64)                6784      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 56,449\n",
      "Trainable params: 56,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 5: use binary target_Y (0 or 1)\n",
    "model_5 = Sequential()\n",
    "\n",
    "model_5.add(Dense(64,\n",
    "                  input_shape=(105,),\n",
    "                  activation=None,\n",
    "                  kernel_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_5.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_5.add(Dense(256, activation=None))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_5.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_5.add(Dense(128, activation=None))\n",
    "# model_4.add(BatchNormalization())\n",
    "model_5.add(LeakyReLU(alpha=0.2))\n",
    "# model_4.add(Dropout(0.2))\n",
    "\n",
    "model_5.add(Dense(1, activation=None))\n",
    "model_5.add(Activation('sigmoid'))\n",
    "\n",
    "# SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)?\n",
    "# model_4.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "\n",
    "# i tried lr=0.001 also but both didn't improve loss\n",
    "# lr=0.001 > lr=0.0001\n",
    "model_5.compile(optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[metrics.binary_accuracy, metrics.mae])\n",
    "\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117001/117001 [==============================] - 506s 4ms/step - loss: 0.4404 - binary_accuracy: 0.8169 - mean_absolute_error: 0.2753 - val_loss: 0.8995 - val_binary_accuracy: 0.5969 - val_mean_absolute_error: 0.4268\n",
      "Epoch 2/10\n",
      "117001/117001 [==============================] - 509s 4ms/step - loss: 0.4395 - binary_accuracy: 0.8200 - mean_absolute_error: 0.2715 - val_loss: 0.9431 - val_binary_accuracy: 0.6155 - val_mean_absolute_error: 0.4095\n",
      "Epoch 3/10\n",
      "117001/117001 [==============================] - 516s 4ms/step - loss: 0.4394 - binary_accuracy: 0.8215 - mean_absolute_error: 0.2683 - val_loss: 1.1854 - val_binary_accuracy: 0.5907 - val_mean_absolute_error: 0.4274\n",
      "Epoch 4/10\n",
      "117001/117001 [==============================] - 504s 4ms/step - loss: 0.4436 - binary_accuracy: 0.8168 - mean_absolute_error: 0.2720 - val_loss: 0.8432 - val_binary_accuracy: 0.6090 - val_mean_absolute_error: 0.4197\n",
      "Epoch 5/10\n",
      "117001/117001 [==============================] - 511s 4ms/step - loss: 0.4567 - binary_accuracy: 0.8086 - mean_absolute_error: 0.2807 - val_loss: 0.9494 - val_binary_accuracy: 0.6517 - val_mean_absolute_error: 0.3746\n",
      "Epoch 6/10\n",
      "117001/117001 [==============================] - 511s 4ms/step - loss: 0.4579 - binary_accuracy: 0.8083 - mean_absolute_error: 0.2805 - val_loss: 1.0737 - val_binary_accuracy: 0.6032 - val_mean_absolute_error: 0.4177\n",
      "Epoch 7/10\n",
      "117001/117001 [==============================] - 514s 4ms/step - loss: 0.4553 - binary_accuracy: 0.8085 - mean_absolute_error: 0.2786 - val_loss: 0.8780 - val_binary_accuracy: 0.6223 - val_mean_absolute_error: 0.4050\n",
      "Epoch 8/10\n",
      "117001/117001 [==============================] - 519s 4ms/step - loss: 0.4636 - binary_accuracy: 0.8033 - mean_absolute_error: 0.2841 - val_loss: 0.8715 - val_binary_accuracy: 0.6333 - val_mean_absolute_error: 0.3934\n",
      "Epoch 9/10\n",
      "117001/117001 [==============================] - 514s 4ms/step - loss: 0.4661 - binary_accuracy: 0.8014 - mean_absolute_error: 0.2851 - val_loss: 0.9353 - val_binary_accuracy: 0.6113 - val_mean_absolute_error: 0.4141\n",
      "Epoch 10/10\n",
      "117001/117001 [==============================] - 521s 4ms/step - loss: 0.4698 - binary_accuracy: 0.7980 - mean_absolute_error: 0.2890 - val_loss: 0.9504 - val_binary_accuracy: 0.5758 - val_mean_absolute_error: 0.4460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbcfede38d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_5\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_5.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 64\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_5.fit_generator(generator=data_generator_binary(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator_binary(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                6784      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 58,241\n",
      "Trainable params: 57,345\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 6: use binary target_Y (0 or 1)\n",
    "model_6 = Sequential()\n",
    "\n",
    "model_6.add(Dense(64,\n",
    "                  input_shape=(105,),\n",
    "                  activation=None,\n",
    "                  kernel_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(LeakyReLU(alpha=0.2))\n",
    "# model_6.add(Dropout(0.2))\n",
    "\n",
    "model_6.add(Dense(256, activation=None))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(LeakyReLU(alpha=0.2))\n",
    "# model_6.add(Dropout(0.2))\n",
    "\n",
    "model_6.add(Dense(128, activation=None))\n",
    "model_6.add(BatchNormalization())\n",
    "model_6.add(LeakyReLU(alpha=0.2))\n",
    "# model_6.add(Dropout(0.2))\n",
    "\n",
    "model_6.add(Dense(1, activation=None))\n",
    "model_6.add(Activation('sigmoid'))\n",
    "\n",
    "# SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)?\n",
    "# model_5.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "model_6.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[metrics.binary_accuracy, metrics.mae])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117001/117001 [==============================] - 952s 8ms/step - loss: 0.3290 - binary_accuracy: 0.8864 - mean_absolute_error: 0.1900 - val_loss: 1.0840 - val_binary_accuracy: 0.5975 - val_mean_absolute_error: 0.4167\n",
      "Epoch 2/10\n",
      "117001/117001 [==============================] - 937s 8ms/step - loss: 0.4297 - binary_accuracy: 0.8226 - mean_absolute_error: 0.2651 - val_loss: 0.8704 - val_binary_accuracy: 0.5967 - val_mean_absolute_error: 0.4391\n",
      "Epoch 3/10\n",
      "117001/117001 [==============================] - 942s 8ms/step - loss: 0.4754 - binary_accuracy: 0.7854 - mean_absolute_error: 0.3025 - val_loss: 0.7443 - val_binary_accuracy: 0.6177 - val_mean_absolute_error: 0.4233\n",
      "Epoch 4/10\n",
      "117001/117001 [==============================] - 945s 8ms/step - loss: 0.4994 - binary_accuracy: 0.7637 - mean_absolute_error: 0.3227 - val_loss: 0.7514 - val_binary_accuracy: 0.6072 - val_mean_absolute_error: 0.4312\n",
      "Epoch 5/10\n",
      "117001/117001 [==============================] - 954s 8ms/step - loss: 0.5174 - binary_accuracy: 0.7491 - mean_absolute_error: 0.3374 - val_loss: 0.7717 - val_binary_accuracy: 0.6264 - val_mean_absolute_error: 0.4091\n",
      "Epoch 6/10\n",
      "117001/117001 [==============================] - 978s 8ms/step - loss: 0.5229 - binary_accuracy: 0.7440 - mean_absolute_error: 0.3418 - val_loss: 0.7076 - val_binary_accuracy: 0.5863 - val_mean_absolute_error: 0.4466\n",
      "Epoch 7/10\n",
      "117001/117001 [==============================] - 959s 8ms/step - loss: 0.5307 - binary_accuracy: 0.7380 - mean_absolute_error: 0.3484 - val_loss: 0.7895 - val_binary_accuracy: 0.6223 - val_mean_absolute_error: 0.4167\n",
      "Epoch 8/10\n",
      "117001/117001 [==============================] - 936s 8ms/step - loss: 0.5329 - binary_accuracy: 0.7358 - mean_absolute_error: 0.3500 - val_loss: 0.7676 - val_binary_accuracy: 0.6156 - val_mean_absolute_error: 0.4182\n",
      "Epoch 9/10\n",
      "117001/117001 [==============================] - 963s 8ms/step - loss: 0.5376 - binary_accuracy: 0.7311 - mean_absolute_error: 0.3542 - val_loss: 0.8405 - val_binary_accuracy: 0.6238 - val_mean_absolute_error: 0.4113\n",
      "Epoch 10/10\n",
      "117001/117001 [==============================] - 963s 8ms/step - loss: 0.5414 - binary_accuracy: 0.7279 - mean_absolute_error: 0.3573 - val_loss: 0.6456 - val_binary_accuracy: 0.6594 - val_mean_absolute_error: 0.4138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3299ae0390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_6\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_6.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "minibatch_size = 64\n",
    "\n",
    "steps_per_epoch = training_data_count // minibatch_size\n",
    "validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "model_6.fit_generator(generator=data_generator_binary(batch_size=minibatch_size, data_type='training'),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=data_generator_binary(batch_size=minibatch_size, data_type='validation'),\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
