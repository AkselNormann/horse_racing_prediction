{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Horse racing prediction  \n",
    "\n",
    "This is an experiment to predict the outcome of horse racing based on past 5 race results, jockey, and trainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prepare mongodb conection\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "\n",
    "db = client.keiba\n",
    "# training_data_Kisyu_Kyusya_1_race_5_with_odds contains data\n",
    "training_data = db.training_data_Kisyu_Kyusya_1_race_5_with_odds\n",
    "# data_models_Kisyu_Kyusya_1_race_5_with_odds contains only std and mean data\n",
    "data_models = db.data_models_Kisyu_Kyusya_1_race_5_with_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get cursor of mongodb\n",
    "all_data_cursor = training_data.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data_count: 9245046\n"
     ]
    }
   ],
   "source": [
    "all_data_count = all_data_cursor.count()\n",
    "print(\"all_data_count: {}\".format(all_data_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get std and mean. we use data_model later\n",
    "mean_and_std = data_models.find_one({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# get all data from mongodb and keep them as numpy.array\n",
    "# target Y is float value\n",
    "#\n",
    "def prepare_training_data():\n",
    "    \n",
    "    input_X = np.zeros(shape=(all_data_count, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(all_data_count, 1), dtype=float)\n",
    "    \n",
    "    idx1 = 0\n",
    "    for data1 in all_data_cursor:\n",
    "        \n",
    "        # normalize x values\n",
    "        for idx2 in data1['input_x_object']:\n",
    "\n",
    "            # get model data which contains mean and std\n",
    "            x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "            mean_name = 'input_x_avg_'+idx2\n",
    "            mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "            std_name = 'input_x_std_'+idx2\n",
    "            std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "            normarized_x = (x1 - mean_value) / std_value\n",
    "            input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "        # normarize y value\n",
    "        y1 = data1['target_y']\n",
    "        y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "        y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "        normalized_y = (y1 - y_mean_value) / y_std_value\n",
    "\n",
    "        target_Y[idx1] = normalized_y\n",
    "        \n",
    "        idx1 = idx1 + 1\n",
    "    \n",
    "    return (input_X, target_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get data actually\n",
    "training_x, training_y = prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save data for future use\n",
    "import pickle\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump((training_x, training_y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# get all data from mongodb and keep them as numpy.array\n",
    "# target y is bainary value 0 or 1. 1 is win, 0 is lose\n",
    "#\n",
    "\n",
    "def prepare_training_data_binary():\n",
    "    \n",
    "    input_X = np.zeros(shape=(all_data_count, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(all_data_count, 1), dtype=float)\n",
    "    \n",
    "    idx1 = 0\n",
    "    for data1 in all_data_cursor:\n",
    "        \n",
    "        # normalize x values\n",
    "        for idx2 in data1['input_x_object']:\n",
    "\n",
    "            # get model data which contains mean and std\n",
    "            x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "            mean_name = 'input_x_avg_'+idx2\n",
    "            mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "            std_name = 'input_x_std_'+idx2\n",
    "            std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "            normarized_x = (x1 - mean_value) / std_value\n",
    "            input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "        # normarize y value\n",
    "        y1 = data1['target_y']\n",
    "        if y1 > 0:\n",
    "            target_Y[idx1] = 1\n",
    "        else:\n",
    "            target_Y[idx1] = 0\n",
    "        \n",
    "        idx1 = idx1 + 1\n",
    "    \n",
    "    return (input_X, target_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get binary version of y\n",
    "training_x_binary, training_y_binary = prepare_training_data_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save data for future use\n",
    "import pickle\n",
    "with open('filename_binary.pickle', 'wb') as handle:\n",
    "    pickle.dump((training_x_binary, training_y_binary), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Restart from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load float version of output y\n",
    "import pickle\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    training_x, training_y = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load float version of output y\n",
    "import pickle\n",
    "with open('filename_binary.pickle', 'rb') as handle:\n",
    "    training_x_binary, training_y_binary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_x, training_y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependancies\n",
    "\n",
    "# allocate 50% of GPU memory (if you like, feel free to change this)\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf \n",
    "\n",
    "# gpu specific\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import keras\n",
    "from keras import metrics, initializers\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Dropout, Dense, LeakyReLU, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               13568     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 79,617\n",
      "Trainable params: 79,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 1: 3 layers, LeakyReLU, and dropout\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(128, input_shape=(105,), activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "# model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(256, activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "# model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(128, activation=None))\n",
    "model_1.add(LeakyReLU(alpha=0.3))\n",
    "# model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(1, activation=None))\n",
    "\n",
    "model_1.compile(optimizer='rmsprop',\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=[metrics.mae])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6656432 samples, validate on 1664109 samples\n",
      "Epoch 1/5\n",
      "6656432/6656432 [==============================] - 414s - loss: 0.1618 - mean_absolute_error: 0.1618 - val_loss: 0.1585 - val_mean_absolute_error: 0.1585\n",
      "Epoch 2/5\n",
      "6656432/6656432 [==============================] - 414s - loss: 0.1610 - mean_absolute_error: 0.1610 - val_loss: 0.1588 - val_mean_absolute_error: 0.1588\n",
      "Epoch 3/5\n",
      "6656432/6656432 [==============================] - 413s - loss: 0.1609 - mean_absolute_error: 0.1609 - val_loss: 0.1587 - val_mean_absolute_error: 0.1587\n",
      "Epoch 4/5\n",
      "6656432/6656432 [==============================] - 411s - loss: 0.1609 - mean_absolute_error: 0.1609 - val_loss: 0.1597 - val_mean_absolute_error: 0.1597\n",
      "Epoch 5/5\n",
      "6656432/6656432 [==============================] - 411s - loss: 0.1609 - mean_absolute_error: 0.1609 - val_loss: 0.1584 - val_mean_absolute_error: 0.1584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7603d7940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_1\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_g1.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "# minibatch_size = 32\n",
    "\n",
    "# steps_per_epoch = training_data_count // minibatch_size\n",
    "# validation_steps = validation_data_count // minibatch_size\n",
    "\n",
    "\n",
    "model_1.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            batch_size=64, \n",
    "            epochs=5, \n",
    "            verbose=1, \n",
    "            callbacks=[checkpointer],\n",
    "            validation_split=0.2,\n",
    "            shuffle=True)\n",
    "\n",
    "\n",
    "# model_1.fit_generator(generator=data_generator(batch_size=minibatch_size, data_type='training'),\n",
    "#                     steps_per_epoch=steps_per_epoch,\n",
    "#                     validation_data=data_generator(batch_size=minibatch_size, data_type='validation'),\n",
    "#                     validation_steps=validation_steps,\n",
    "#                     epochs=20,\n",
    "#                     callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## evaluate model  \n",
    "inference to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test[2:12]:[[ 0.02262738]\n",
      " [ 0.08081207]\n",
      " [ 0.04525476]\n",
      " [-0.02262738]\n",
      " [ 0.08404455]\n",
      " [-0.00646497]\n",
      " [-0.00969745]\n",
      " [-0.04202228]\n",
      " [-0.00323248]\n",
      " [ 0.04525476]]\n",
      "y1:[[  7.]\n",
      " [ 25.]\n",
      " [ 14.]\n",
      " [ -7.]\n",
      " [ 26.]\n",
      " [ -2.]\n",
      " [ -3.]\n",
      " [-13.]\n",
      " [ -1.]\n",
      " [ 14.]]\n",
      "\n",
      "pred_normalized:[[-0.03939489]\n",
      " [-0.01960922]\n",
      " [-0.02069423]\n",
      " [-0.00197923]\n",
      " [ 0.23016065]\n",
      " [-0.00114011]\n",
      " [-0.01365444]\n",
      " [-0.01148138]\n",
      " [-0.01751973]\n",
      " [-0.0013935 ]]\n",
      "pred1:[[-12.18719101]\n",
      " [ -6.06630373]\n",
      " [ -6.40196085]\n",
      " [ -0.61229342]\n",
      " [ 71.20243073]\n",
      " [ -0.35270301]\n",
      " [ -4.22413254]\n",
      " [ -3.55187583]\n",
      " [ -5.41989899]\n",
      " [ -0.43109414]]\n",
      "aaa: [[ -8.53103371e+01]\n",
      " [ -1.51657593e+02]\n",
      " [ -8.96274519e+01]\n",
      " [  4.28605396e+00]\n",
      " [  1.85126320e+03]\n",
      " [  7.05406010e-01]\n",
      " [  1.26723976e+01]\n",
      " [  4.61743858e+01]\n",
      " [  5.41989899e+00]\n",
      " [ -6.03531796e+00]]\n",
      "aaa : [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "10\n",
      "accuracy1:60.0\n"
     ]
    }
   ],
   "source": [
    "# modify y_test to binary data\n",
    "\n",
    "# y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "# y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "\n",
    "# y1 = y_test * y_std_value + y_mean_value\n",
    "\n",
    "# pred_normalized = model_1.predict(x_train)\n",
    "# pred1 = pred_normalized * y_std_value + y_mean_value\n",
    "\n",
    "# # multiply prediction and actuall value. if sing is same the result should be positive\n",
    "# check_1 = pred1 * y1\n",
    "\n",
    "# idx = 0\n",
    "# for item in check_1:\n",
    "#     if item * y_train[idx] > 0:\n",
    "#         check_1[idx] = 1\n",
    "#     else:\n",
    "#         check_1[idx] = 0\n",
    "    \n",
    "#     idx = idx + 1\n",
    "\n",
    "\n",
    "# accuracy1 = 100*np.sum(check_1) / len(check_1)\n",
    "# print(\"accuracy1:{}\".format(accuracy1))\n",
    "\n",
    "# normalized_y = (y1 - y_mean_value) / y_std_value\n",
    "\n",
    "y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "\n",
    "# x_test[2:3]\n",
    "\n",
    "y1 = y_test[2:12] * y_std_value + y_mean_value\n",
    "\n",
    "pred_normalized = model_1.predict(x_test[2:12])\n",
    "pred1 = pred_normalized * y_std_value + y_mean_value\n",
    "\n",
    "print(\"y_test[2:12]:{}\".format(y_test[2:12]))\n",
    "print(\"y1:{}\".format(y1))\n",
    "print(\"\")\n",
    "print(\"pred_normalized:{}\".format(pred_normalized))\n",
    "print(\"pred1:{}\".format(pred1))\n",
    "\n",
    "aaa = pred1 * y1\n",
    "print(\"aaa: {}\".format(aaa))\n",
    "\n",
    "idx = 0\n",
    "for item in aaa:\n",
    "    if item > 0:\n",
    "        aaa[idx] = 1\n",
    "    else:\n",
    "        aaa[idx] = 0\n",
    "    \n",
    "    idx = idx + 1\n",
    "\n",
    "print(\"aaa : {}\".format(aaa))\n",
    "print(len(aaa))\n",
    "\n",
    "accuracy1 = 100*np.sum(aaa) / len(aaa)\n",
    "print(\"accuracy1:{}\".format(accuracy1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               13568     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 79,617\n",
      "Trainable params: 79,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 2:\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Dense(128, \n",
    "                  input_shape=(105,), \n",
    "                  kernel_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  bias_initializer=initializers.TruncatedNormal(mean=0.0, stddev=0.01, seed=None),\n",
    "                  activation=None))\n",
    "# model_2.add(BatchNormalization())\n",
    "model_2.add(LeakyReLU(alpha=0.3))\n",
    "# model_2.add(Dropout(0.2))\n",
    "\n",
    "# model_2.add(Dense(256, activation=None))\n",
    "# # model_2.add(BatchNormalization())\n",
    "# model_2.add(LeakyReLU(alpha=0.3))\n",
    "\n",
    "model_2.add(Dense(256, activation=None))\n",
    "# model_2.add(BatchNormalization())\n",
    "model_2.add(LeakyReLU(alpha=0.3))\n",
    "# model_2.add(Dropout(0.2))\n",
    "\n",
    "model_2.add(Dense(128, activation=None))\n",
    "# model_2.add(BatchNormalization())\n",
    "model_2.add(LeakyReLU(alpha=0.3))\n",
    "# model_2.add(Dropout(0.2))\n",
    "\n",
    "model_2.add(Dense(1, activation=None))\n",
    "model_2.add(Activation('sigmoid'))\n",
    "\n",
    "# Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model_2.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7396036 samples, validate on 1849010 samples\n",
      "Epoch 1/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5973 - binary_accuracy: 0.6770 - val_loss: 0.5924 - val_binary_accuracy: 0.6825\n",
      "Epoch 2/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5970 - binary_accuracy: 0.6773 - val_loss: 0.5923 - val_binary_accuracy: 0.6814\n",
      "Epoch 3/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5967 - binary_accuracy: 0.6776 - val_loss: 0.5915 - val_binary_accuracy: 0.6819\n",
      "Epoch 4/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5964 - binary_accuracy: 0.6778 - val_loss: 0.5933 - val_binary_accuracy: 0.6819\n",
      "Epoch 5/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5962 - binary_accuracy: 0.6779 - val_loss: 0.5908 - val_binary_accuracy: 0.6826\n",
      "Epoch 6/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5960 - binary_accuracy: 0.6781 - val_loss: 0.5909 - val_binary_accuracy: 0.6828\n",
      "Epoch 7/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5958 - binary_accuracy: 0.6784 - val_loss: 0.5926 - val_binary_accuracy: 0.6816\n",
      "Epoch 8/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5957 - binary_accuracy: 0.6784 - val_loss: 0.5909 - val_binary_accuracy: 0.6832\n",
      "Epoch 9/20\n",
      "7396036/7396036 [==============================] - 286s - loss: 0.5955 - binary_accuracy: 0.6785 - val_loss: 0.5919 - val_binary_accuracy: 0.6825\n",
      "Epoch 10/20\n",
      "7396036/7396036 [==============================] - 285s - loss: 0.5954 - binary_accuracy: 0.6787 - val_loss: 0.5943 - val_binary_accuracy: 0.6804\n",
      "Epoch 11/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5953 - binary_accuracy: 0.6788 - val_loss: 0.5904 - val_binary_accuracy: 0.6829\n",
      "Epoch 12/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5952 - binary_accuracy: 0.6789 - val_loss: 0.5909 - val_binary_accuracy: 0.6821\n",
      "Epoch 13/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5951 - binary_accuracy: 0.6789 - val_loss: 0.5905 - val_binary_accuracy: 0.6828\n",
      "Epoch 14/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5950 - binary_accuracy: 0.6790 - val_loss: 0.5912 - val_binary_accuracy: 0.6833\n",
      "Epoch 15/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5949 - binary_accuracy: 0.6791 - val_loss: 0.5900 - val_binary_accuracy: 0.6836\n",
      "Epoch 16/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5948 - binary_accuracy: 0.6792 - val_loss: 0.5946 - val_binary_accuracy: 0.6831\n",
      "Epoch 17/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5948 - binary_accuracy: 0.6790 - val_loss: 0.5898 - val_binary_accuracy: 0.6834\n",
      "Epoch 18/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5947 - binary_accuracy: 0.6792 - val_loss: 0.5908 - val_binary_accuracy: 0.6827\n",
      "Epoch 19/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5946 - binary_accuracy: 0.6794 - val_loss: 0.5899 - val_binary_accuracy: 0.6836\n",
      "Epoch 20/20\n",
      "7396036/7396036 [==============================] - 284s - loss: 0.5946 - binary_accuracy: 0.6794 - val_loss: 0.5912 - val_binary_accuracy: 0.6832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e44b43b00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model_2\n",
    "\n",
    "# add checkpointer\n",
    "save_model_name = \"keiba_model_g2.h5\"\n",
    "checkpointer = ModelCheckpoint(filepath='results/'+save_model_name, verbose=0)\n",
    "\n",
    "# training_x_binary, training_y_binary\n",
    "\n",
    "model_2.fit(x=training_x_binary, \n",
    "            y=training_y_binary, \n",
    "            batch_size=128, \n",
    "            epochs=20, \n",
    "            verbose=1, \n",
    "            callbacks=[checkpointer],\n",
    "            validation_split=0.2,\n",
    "            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get batch as generator: not used here\n",
    "def data_generator(batch_size, data_type):\n",
    "    \n",
    "    input_X = np.zeros(shape=(batch_size, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(batch_size, 1), dtype=float)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for idx1 in range(batch_size):\n",
    "            \n",
    "            # get one row\n",
    "            data1 = None\n",
    "            if data_type == 'validation':\n",
    "                data1 = validation_data_cursor.next()\n",
    "            else:\n",
    "                data1 = training_data_cursor.next()\n",
    "\n",
    "            # normalize x values\n",
    "            for idx2 in data1['input_x_object']:\n",
    "\n",
    "                # get model data which contains mean and std\n",
    "                x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "                mean_name = 'input_x_avg_'+idx2\n",
    "                mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "                std_name = 'input_x_std_'+idx2\n",
    "                std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "                normarized_x = (x1 - mean_value) / std_value\n",
    "                input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "            # normarize y value\n",
    "            y1 = data1['target_y']\n",
    "            y_mean_value = mean_and_std['mean_and_std']['target_y_mean']\n",
    "            y_std_value = mean_and_std['mean_and_std']['target_y_stddev']\n",
    "            normalized_y = (y1 - y_mean_value) / y_std_value\n",
    "\n",
    "            target_Y[idx1] = normalized_y\n",
    "\n",
    "            yield (input_X, target_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get batch (y is binary data) as generator: not used here\n",
    "def data_generator_binary(batch_size, data_type):\n",
    "    \n",
    "    input_X = np.zeros(shape=(batch_size, 105), dtype=float)\n",
    "    target_Y = np.zeros(shape=(batch_size, 1), dtype=float)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for idx1 in range(batch_size):\n",
    "            \n",
    "            # get one row\n",
    "            data1 = None\n",
    "            if data_type == 'validation':\n",
    "                data1 = validation_data_cursor.next()\n",
    "            else:\n",
    "                data1 = training_data_cursor.next()\n",
    "\n",
    "            # normalize x values\n",
    "            for idx2 in data1['input_x_object']:\n",
    "\n",
    "                # get model data which contains mean and std\n",
    "                x1 = data1['input_x_object'][idx2]\n",
    "\n",
    "                mean_name = 'input_x_avg_'+idx2\n",
    "                mean_value = mean_and_std['mean_and_std'][mean_name]\n",
    "\n",
    "                std_name = 'input_x_std_'+idx2\n",
    "                std_value = mean_and_std['mean_and_std'][std_name]\n",
    "\n",
    "                normarized_x = (x1 - mean_value) / std_value\n",
    "                input_X[idx1, int(idx2)] = normarized_x\n",
    "\n",
    "            # normarize y value\n",
    "            y1 = data1['target_y']\n",
    "            if y1 >= 0:\n",
    "                target_Y[idx1] = 1\n",
    "            else:\n",
    "                target_Y[idx1] = 0\n",
    "\n",
    "            yield (input_X, target_Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
